# Dueling Posterior Sampling ‚Äì Versi√≥n Adaptada con Preferencias B-Pref

Este repositorio contiene una versi√≥n adaptada y extendida del c√≥digo original de:

**Dueling Posterior Sampling for Preference-Based Reinforcement Learning**
*Conference on Uncertainty in Artificial Intelligence (UAI), 2020*
Ellen Novoseller, Yibing Wei, Yanan Sui, Yisong Yue y Joel W. Burdick
[Paper DPS](https://arxiv.org/abs/1908.01289)

Implementando un profesor simulado seg√∫n las descripciones del benchmark:

**B-Pref: Benchmarking Preference-Based Reinforcement Learning**
*NeurIPS 2021 Datasets and Benchmarks Track*
Kimin Lee, Laura Smith, Anca Dragan, Pieter Abbeel
[Paper B-Pref] (https://openreview.net/pdf?id=ps95-mkHF_)

---

## Contenidos del Repositorio

Este c√≥digo fue desarrollado como parte de un trabajo de titulaci√≥n para la carrera de Ingenier√≠a Civil Electr√≥nica, con el objetivo de:

* Replicar el algoritmo Dueling Posterior Sampling (DPS) para Aprendizaje Reforzado Basado en Preferencias.
* Implementar una versi√≥n modificada de los entornos para entregar preferencias utilizando el benchmark BPref.
* Se implement√≥ manualmente un profesor simulado inspirado en el benchmark B-Pref (Lee et al., NeurIPS 2021), con el objetivo de evaluar la robustez del algoritmo frente a preferencias imperfectas. Esto permite emular comportamientos humanos no racionales, como inconsistencias o ruido en las comparaciones.

---

## Sobre Dueling Posterior Sampling

DPS es un enfoque bayesiano que permite aprender simult√°neamente:

* La din√°mica del entorno.
* La funci√≥n de utilidad que origina el feedback en forma de preferencias (en lugar de recompensas absolutas).

Se presentan varias estrategias de asignaci√≥n de cr√©dito para interpretar esas preferencias, incluyendo:

* Regresi√≥n lineal bayesiana
* Regresi√≥n log√≠stica bayesiana
* Modelos de preferencia con proceso gaussiano (GP)
* Regresi√≥n con GP

---

## üìÅ Estructura del repositorio

* `Learning_algorithms/`: Algoritmos implementados (DPS, EPMC, PSRL).
* `Envs/`: Entornos simulados, incluyendo versiones modificadas para BPref.
* Scripts principales en el formato `algoritmo_in_entorno.py` para ejecutar los distintos algoritmos de aprendizaje en la versi√≥n BPref de cada entorno.

---

## Reproducci√≥n de Resultados

Para reproducir el entorno de desarrollo:

```
conda env create -f environment.yml
conda activate bpref-dps-env
```

Luego se ejecuta el script que corresponde con el algoritmo de aprendizaje que se quiere probar en el ambiente que corresponda. La versi√≥n actual tiene todos los ambientes implementados para entregar preferencias B-Pref, pero solo se adjuntan scripts para correr los experimentos en RiverSwim.

---

## üìÑ Referencias

\[1] E. Novoseller, Y. Wei, Y. Sui, Y. Yue y J. Burdick. *Dueling Posterior Sampling for Preference-Based Reinforcement Learning*. arXiv:1908.01289, 2020.
\[2] C. Wirth y J. F√ºrnkranz. *A policy iteration algorithm for learning from preference-based feedback*, 2013.
\[3] C. Wirth. *Efficient Preference-Based Reinforcement Learning*. Tesis doctoral, 2017.
\[4] I. Osband, D. Russo y B. Van Roy. *(More) efficient reinforcement learning via posterior sampling*, 2013.
\[5] K. Lee, L. Smith, A. Dragan y P. Abbeel. *B-Pref: Benchmarking Preference-Based Reinforcement Learning*. NeurIPS Datasets and Benchmarks Track, 2021.
[Repositorio B-Pref](https://github.com/rll-research/B-Pref)
